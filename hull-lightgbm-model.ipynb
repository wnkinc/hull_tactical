{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732abc6f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-11T19:44:05.708520Z",
     "iopub.status.busy": "2025-11-11T19:44:05.708123Z",
     "iopub.status.idle": "2025-11-11T19:44:07.825630Z",
     "shell.execute_reply": "2025-11-11T19:44:07.824255Z"
    },
    "papermill": {
     "duration": 2.123839,
     "end_time": "2025-11-11T19:44:07.827536",
     "exception": false,
     "start_time": "2025-11-11T19:44:05.703697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab0f68fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T19:44:07.834573Z",
     "iopub.status.busy": "2025-11-11T19:44:07.834071Z",
     "iopub.status.idle": "2025-11-11T19:47:33.139594Z",
     "shell.execute_reply": "2025-11-11T19:47:33.138167Z"
    },
    "papermill": {
     "duration": 205.311574,
     "end_time": "2025-11-11T19:47:33.141569",
     "exception": false,
     "start_time": "2025-11-11T19:44:07.829995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "\n",
    "# --- Kaggle Evaluation API Interface (Mandatory) ---\n",
    "try:\n",
    "    import kaggle_evaluation.default_inference_server as inference_server\n",
    "except ImportError:\n",
    "    # Mock definitions for local testing environment\n",
    "    class MockInferenceServer:\n",
    "        def __init__(self, predict_fn):\n",
    "            self.predict_fn = predict_fn\n",
    "        def serve(self):\n",
    "            logging.info(\"Mock Inference Server Running...\")\n",
    "        def run_local_gateway(self, path):\n",
    "            logging.info(f\"Mock Local Gateway Running with path: {path}\")\n",
    "    inference_server = MockInferenceServer\n",
    "\n",
    "# --- Global Model Variables ---\n",
    "MODEL_LGBM = None\n",
    "MODEL_XGB = None\n",
    "TRAIN_COLS = None\n",
    "TRAIN_MEANS = None\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# --- Feature Engineering and Selection (Simplified) ---\n",
    "\n",
    "def preprocess_data(df: pl.DataFrame, is_training: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standard preprocessing for the financial time series data, now simplified \n",
    "    to reduce overfitting from overly long-range features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Polars Feature Engineering (Time Series Features) ---\n",
    "    \n",
    "    if 'date_id' in df.columns:\n",
    "        df = df.with_columns(\n",
    "            (pl.col('date_id') % 5).alias('day_of_cycle') # Proxy for Day-of-Week\n",
    "        )\n",
    "    \n",
    "    # REDUCTION: Only use shorter, more relevant rolling windows\n",
    "    ROLLING_WINDOWS = [5, 10]\n",
    "    BASE_FEATURES = ['M1', 'E1', 'V1', 'S1', 'T1', 'P1', 'D1']\n",
    "\n",
    "    expressions = []\n",
    "    \n",
    "    # Simple Lag Features for Momentum\n",
    "    LAG_WINDOWS = [1, 5]\n",
    "    for lag in LAG_WINDOWS:\n",
    "        for col in BASE_FEATURES:\n",
    "            if col in df.columns:\n",
    "                expressions.append(\n",
    "                    pl.col(col).shift(lag).alias(f'{col}_lag_{lag}')\n",
    "                )\n",
    "    \n",
    "    # Rolling Mean and Std Dev Features\n",
    "    for window in ROLLING_WINDOWS:\n",
    "        for col in BASE_FEATURES:\n",
    "            if col in df.columns:\n",
    "                expressions.append(\n",
    "                    pl.col(col).rolling_mean(window_size=window, min_samples=1).alias(f'{col}_roll_mean_{window}')\n",
    "                )\n",
    "                expressions.append(\n",
    "                    pl.col(col).rolling_std(window_size=window, min_samples=1).alias(f'{col}_roll_std_{window}')\n",
    "                )\n",
    "    \n",
    "    if expressions:\n",
    "        df = df.with_columns(expressions)\n",
    "        \n",
    "    # --- Convert to Pandas and create Interaction and EMA Features ---\n",
    "    pdf = df.to_pandas()\n",
    "    \n",
    "    # REDUCTION: Only use shorter EMAs\n",
    "    EMA_WINDOWS = [10, 30] \n",
    "    for window in EMA_WINDOWS:\n",
    "        for col in BASE_FEATURES:\n",
    "            if col in pdf.columns:\n",
    "                pdf[f'{col}_ema_{window}'] = pdf[col].ewm(span=window, adjust=False).mean()\n",
    "    \n",
    "    # Interaction Features (Ratios and Differences) - Kept\n",
    "    FEATURE_PAIRS = [('M1', 'M2'), ('E1', 'E2'), ('V1', 'V2'),\n",
    "                     ('S1', 'S2'), ('T1', 'T2'), ('P1', 'P2'), ('D1', 'D2')]\n",
    "\n",
    "    for col1, col2 in FEATURE_PAIRS:\n",
    "        if col1 in pdf.columns and col2 in pdf.columns:\n",
    "            pdf[f'{col1}_div_{col2}'] = pdf[col1] / (pdf[col2].replace(0, 1e-6) + 1e-6)\n",
    "            pdf[f'{col1}_minus_{col2}'] = pdf[col1] - pdf[col2]\n",
    "            \n",
    "    # Quadratic Features (Non-Linearity) - Kept\n",
    "    for col in BASE_FEATURES:\n",
    "        if col in pdf.columns:\n",
    "            pdf[f'{col}_sq'] = pdf[col] ** 2\n",
    "\n",
    "    # Drop non-feature columns\n",
    "    EXCLUDE_FINAL_COLS = ['date_id', 'forward_returns', 'risk_free_rate',\n",
    "                          'market_forward_excess_returns', 'is_scored',\n",
    "                          'lagged_forward_returns', 'lagged_risk_free_rate',\n",
    "                          'lagged_market_forward_excess_returns']\n",
    "                          \n",
    "    final_cols = [col for col in pdf.columns if col not in EXCLUDE_FINAL_COLS]\n",
    "    \n",
    "    return pdf[final_cols]\n",
    "\n",
    "# --- Core Prediction and Allocation Logic ---\n",
    "\n",
    "def train_model(train_df: pl.DataFrame):\n",
    "    \"\"\"\n",
    "    Trains a LightGBM and an XGBoost model with enhanced regularization.\n",
    "    \"\"\"\n",
    "    global MODEL_LGBM, MODEL_XGB, TRAIN_COLS, TRAIN_MEANS\n",
    "    \n",
    "    # Target is binary: Predict 1 if the return is positive, 0 otherwise.\n",
    "    y_train = (train_df['market_forward_excess_returns'].to_numpy() > 0).astype(int)\n",
    "    \n",
    "    # Prepare Features\n",
    "    X_train_pd = preprocess_data(train_df, is_training=True)\n",
    "    \n",
    "    TRAIN_COLS = list(X_train_pd.columns)\n",
    "    \n",
    "    # Impute Data Manually\n",
    "    TRAIN_MEANS = X_train_pd.mean()\n",
    "    X_train_pd = X_train_pd.fillna(TRAIN_MEANS)\n",
    "\n",
    "    # --- 1. LightGBM Model Training (Increased Regularization) ---\n",
    "    regressor_lgbm = lgb.LGBMClassifier( \n",
    "        objective='binary',\n",
    "        n_estimators=3500,             \n",
    "        learning_rate=0.01,            \n",
    "        num_leaves=127,                # REDUCED complexity (was 255)\n",
    "        subsample=0.8,                \n",
    "        colsample_bytree=0.8,         \n",
    "        random_state=42,\n",
    "        n_jobs=4,\n",
    "        reg_lambda=3.0,                # INCREASED L2 regularization (was 2.0)\n",
    "        reg_alpha=0.5,                 # INCREASED L1 regularization (was 0.3)\n",
    "        min_child_samples=20,          \n",
    "        boosting_type='gbdt'\n",
    "    )\n",
    "    logging.info(\"Starting LightGBM training...\")\n",
    "    regressor_lgbm.fit(X_train_pd, y_train)\n",
    "    MODEL_LGBM = regressor_lgbm\n",
    "    logging.info(\"LightGBM training complete.\")\n",
    "\n",
    "    # --- 2. XGBoost Model Training (Increased Regularization) ---\n",
    "    regressor_xgb = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        n_estimators=3000,             \n",
    "        learning_rate=0.01,\n",
    "        max_depth=7,                   # REDUCED tree depth (was 9)\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=0.2,                     \n",
    "        reg_lambda=3.0,                # INCREASED L2 regularization (was 2.0)\n",
    "        reg_alpha=0.5,                 # INCREASED L1 regularization (was 0.3)\n",
    "        random_state=42,\n",
    "        n_jobs=4,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    logging.info(\"Starting XGBoost training...\")\n",
    "    regressor_xgb.fit(X_train_pd, y_train)\n",
    "    MODEL_XGB = regressor_xgb\n",
    "    logging.info(\"XGBoost training complete.\")\n",
    "\n",
    "def convert_prediction_to_allocation(predicted_probability: float) -> float:\n",
    "    \"\"\"\n",
    "    Converts the model's predicted probability of a positive return (P)\n",
    "    into the required allocation size (0.0 to 2.0).\n",
    "    \n",
    "    Multiplier is reduced for lower risk.\n",
    "    \"\"\"\n",
    "    \n",
    "    CONFIDENCE_MULTIPLIER = 3.0 # REDUCED from 4.0 for less aggressive betting\n",
    "\n",
    "    # 1. Calculate the 'Edge' or deviation from neutral (0.5)\n",
    "    edge = predicted_probability - 0.5\n",
    "    \n",
    "    # 2. Apply the Tanh scaling. \n",
    "    scaled_edge = np.tanh(CONFIDENCE_MULTIPLIER * edge)\n",
    "    \n",
    "    # 3. Map the scaled edge back to the final allocation range [0.0, 2.0]\n",
    "    final_allocation = 1.0 + scaled_edge\n",
    "    \n",
    "    # Clip the result to the required range [0.0, 2.0]\n",
    "    final_allocation = np.clip(final_allocation, 0.0, 2.0)\n",
    "    \n",
    "    return float(final_allocation)\n",
    "\n",
    "# --- The Required Kaggle Inference Function ---\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    The main inference function called by the Kaggle evaluation API.\n",
    "    Calculates predictions from both LGBM and XGBoost and uses a 60/40 blend.\n",
    "    \"\"\"\n",
    "    global MODEL_LGBM, MODEL_XGB, TRAIN_COLS, TRAIN_MEANS\n",
    "    \n",
    "    is_mock_run = not os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n",
    "    default_return = (1.0, 0.5) if is_mock_run else 1.0\n",
    "    \n",
    "    if MODEL_LGBM is None or MODEL_XGB is None:\n",
    "        # Load the full training set for the very first call to train the model\n",
    "        train_path = os.path.expanduser('~/Documents/kaggle/hull_tactical/data/train.csv')\n",
    "        \n",
    "        try:\n",
    "            train_df = pl.read_csv(train_path, try_parse_dates=True, infer_schema_length=100000)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Could not load train.csv: {e}. Returning neutral allocation.\")\n",
    "            return default_return\n",
    "\n",
    "        train_model(train_df)\n",
    "        \n",
    "        if MODEL_LGBM is None or MODEL_XGB is None:\n",
    "            logging.error(\"Model training failed. Returning neutral allocation.\")\n",
    "            return default_return\n",
    "\n",
    "    # Ensure we only use the features the models were trained on\n",
    "    try:\n",
    "        # Preprocess the test data to create the same interaction features\n",
    "        X_test_pd = preprocess_data(test)\n",
    "        \n",
    "        # Identify missing features and fill with NaN\n",
    "        missing_cols = set(TRAIN_COLS) - set(X_test_pd.columns)\n",
    "        for col in missing_cols:\n",
    "            X_test_pd[col] = np.nan\n",
    "            \n",
    "        X_test_pd = X_test_pd[TRAIN_COLS] # Reorder and select\n",
    "        \n",
    "        # IMPUTATION STEP: using TRAIN_MEANS\n",
    "        if TRAIN_MEANS is not None:\n",
    "              X_test_pd = X_test_pd.fillna(TRAIN_MEANS)\n",
    "        else:\n",
    "              X_test_pd = X_test_pd.fillna(X_test_pd.mean())\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Feature selection failed in predict: {e}\")\n",
    "        return default_return\n",
    "\n",
    "    # Predict the probability of positive excess return (P(return > 0))\n",
    "    try:\n",
    "        # Get individual model predictions\n",
    "        p_lgbm = MODEL_LGBM.predict_proba(X_test_pd)[-1, 1]\n",
    "        p_xgb = MODEL_XGB.predict_proba(X_test_pd)[-1, 1]\n",
    "\n",
    "        # --- ENSEMBLE BLEND (60/40 Average) ---\n",
    "        # Giving 60% weight to the more stable LGBM model\n",
    "        p_positive_blended = (0.6 * p_lgbm) + (0.4 * p_xgb)\n",
    "        \n",
    "        # Convert blended prediction into the final allocation\n",
    "        final_allocation = convert_prediction_to_allocation(p_positive_blended)\n",
    "        \n",
    "        # Return the allocation (and probability for mock testing)\n",
    "        if is_mock_run:\n",
    "              return (final_allocation, p_positive_blended)\n",
    "        \n",
    "        return final_allocation\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Inference failed: {e}. Returning neutral allocation.\")\n",
    "        return default_return\n",
    "\n",
    "# --- Visualization Helper ---\n",
    "def plot_results(results_df: pd.DataFrame):\n",
    "    \"\"\"Generates a plot of the predicted returns and allocations.\"\"\"\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # Plot Predicted Probability (left axis)\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Simulated Day')\n",
    "    ax1.set_ylabel('Blended Predicted Probability (P > 0)', color=color)\n",
    "    ax1.plot(results_df['day'], results_df['predicted_return'], color=color, label='Blended P(Up)', alpha=0.6)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax1.axhline(0.5, color='orange', linestyle='--', label='Neutral P(0.5)')\n",
    "\n",
    "    # Plot Allocation (right axis)\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Final Allocation (0.0 to 2.0)', color=color)  \n",
    "    ax2.plot(results_df['day'], results_df['allocation'], color=color, label='Final Allocation', linewidth=2)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.axhline(1.0, color='gray', linestyle=':', label='Neutral (1.0)')\n",
    "\n",
    "    # Title and Final Touches\n",
    "    fig.suptitle('Robust LGBM (60%) + XGBoost (40%) Ensemble Mock Prediction')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Mock Test Runner ---\n",
    "def run_mock_test_and_visualize():\n",
    "    \"\"\"Simulates 50 days of inference for visualization.\"\"\"\n",
    "    \n",
    "    # Features used for the mock test\n",
    "    MOCK_FEATURES = [\n",
    "        'D1', 'D2', 'E1', 'E2', 'V1', 'V2', 'S1', 'S2', 'M1', 'M2', 'T1', 'T2', 'P1', 'P2'\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # 1. Simulate training (call predict once)\n",
    "    mock_data_init = {col: [np.random.rand()] for col in MOCK_FEATURES}\n",
    "    mock_data_init['market_forward_excess_returns'] = [np.random.uniform(-0.01, 0.01)]\n",
    "    mock_data_init['date_id'] = [1000]\n",
    "    mock_test_df_init = pl.DataFrame(mock_data_init)\n",
    "    \n",
    "    logging.info(\"Starting initial predict call (triggers LGBM + XGBoost training simulation)...\")\n",
    "    \n",
    "    # Call predict() to trigger model training. We discard the first result.\n",
    "    _ = predict(mock_test_df_init)\n",
    "    logging.info(\"Ensemble models are now trained and ready for inference.\")\n",
    "\n",
    "    # 2. Simulate 50 days of real-time inference\n",
    "    NUM_SIMULATION_DAYS = 50\n",
    "    for day in range(NUM_SIMULATION_DAYS):\n",
    "        # Generate random feature values for the day\n",
    "        mock_day_data = {\n",
    "            c: [np.random.uniform(0.1, 0.9) if c == 'M1' else np.random.rand()]\n",
    "            for c in MOCK_FEATURES\n",
    "        }\n",
    "        mock_day_data['market_forward_excess_returns'] = [np.random.uniform(-0.01, 0.01)]\n",
    "        mock_day_data['date_id'] = [1001 + day]\n",
    "        mock_test_df_day = pl.DataFrame(mock_day_data)\n",
    "        \n",
    "        # Predict returns the tuple (allocation, raw_prediction) in mock mode\n",
    "        allocation, p_positive_blended = predict(mock_test_df_day)\n",
    "        \n",
    "        results.append({\n",
    "            'day': day + 1,\n",
    "            'predicted_return': p_positive_blended, # Storing blended probability here\n",
    "            'allocation': allocation\n",
    "        })\n",
    "\n",
    "    # 3. Process and Plot Results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    logging.info(f\"\\n--- MOCK TEST SIMULATION SUMMARY (50 Days) ---\")\n",
    "    logging.info(f\"Mean Blended Predicted Probability (P > 0): {results_df['predicted_return'].mean():.6f}\")\n",
    "    logging.info(f\"Mean Final Allocation: {results_df['allocation'].mean():.4f}\")\n",
    "    logging.info(f\"Min/Max Allocation: {results_df['allocation'].min():.4f} / {results_df['allocation'].max():.4f}\")\n",
    "    logging.info(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Print the plot output (this will render the visualization)\n",
    "    plot_results(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04883950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting LightGBM training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4662, number of negative: 4359\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36117\n",
      "[LightGBM] [Info] Number of data points in the train set: 9021, number of used features: 160\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.516794 -> initscore=0.067202\n",
      "[LightGBM] [Info] Start training from score 0.067202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:LightGBM training complete.\n",
      "INFO:root:Starting XGBoost training...\n",
      "/home/wes/Documents/kaggle/hull_tactical/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [13:53:32] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "INFO:root:XGBoost training complete.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "train_path = Path.home() / \"Documents/kaggle/hull_tactical/data/train.csv\"\n",
    "train_df = pl.read_csv(train_path, try_parse_dates=True, infer_schema_length=100000)\n",
    "\n",
    "train_model(train_df)  # this will set MODEL_LGBM, MODEL_XGB, TRAIN_COLS, TRAIN_MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a6576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             feature          gain  split\n",
      "63                P4  14013.633728  12286\n",
      "62                P3  13118.381835  11071\n",
      "50                M4  13083.233308  10805\n",
      "136        D1_ema_10  12946.348183  15252\n",
      "142        D1_ema_30  10294.286439  13181\n",
      "66                P7  10289.261115   8510\n",
      "49                M3   9885.359236   8493\n",
      "87                V3   9732.671481   8770\n",
      "19               E19   9660.302122   8466\n",
      "114    S1_roll_std_5   8866.907653   8001\n",
      "59               P12   8842.500311   8068\n",
      "76                S5   8776.039084   7394\n",
      "60               P13   8510.186291   7449\n",
      "73                S2   8333.188981   7003\n",
      "65                P6   7563.545835   6412\n",
      "64                P5   7423.485925   6693\n",
      "149        S1_div_S2   6971.983711   6640\n",
      "54                M8   6654.677641   5579\n",
      "126   S1_roll_std_10   6640.954800   5915\n",
      "48                M2   6498.357515   5249\n",
      "89                V5   6431.189268   5913\n",
      "77                S6   6396.754487   5280\n",
      "80                S9   6319.017640   5216\n",
      "72               S12   6289.178431   5899\n",
      "44               M15   6242.475723   5578\n",
      "85               V13   5815.270671   4805\n",
      "23                E4   5795.068156   5091\n",
      "75                S4   5742.799351   4702\n",
      "70               S10   5731.676368   4805\n",
      "116    P1_roll_std_5   5629.533410   5240\n",
      "128   P1_roll_std_10   5407.347752   4517\n",
      "14               E14   5274.080127   4321\n",
      "94      day_of_cycle   5258.128853   5635\n",
      "29                I1   5079.281286   4616\n",
      "13               E13   4711.450885   4083\n",
      "71               S11   4569.147066   4011\n",
      "150      S1_minus_S2   4292.156974   3857\n",
      "31                I3   4251.989777   3820\n",
      "35                I7   4204.958596   3571\n",
      "78                S7   4186.550843   3693\n",
      "34                I6   4171.788595   3901\n",
      "68                P9   4156.163413   3564\n",
      "53                M7   4118.431206   3628\n",
      "88                V4   3901.325391   3391\n",
      "11               E11   3798.104189   3097\n",
      "39               M10   3795.867377   3027\n",
      "86                V2   3794.544373   3245\n",
      "105         P1_lag_5   3759.747616   3165\n",
      "25                E6   3696.739629   2995\n",
      "158            S1_sq   3646.199102   3317\n",
      "148      V1_minus_V2   3631.666874   3363\n",
      "151        P1_div_P2   3463.677838   3224\n",
      "12               E12   3458.325820   2947\n",
      "91                V7   3443.609136   2841\n",
      "30                I2   3439.003275   3078\n",
      "147        V1_div_V2   3303.735005   2893\n",
      "40               M11   3277.918890   2783\n",
      "27                E8   3277.498449   3037\n",
      "55                M9   3254.711007   2576\n",
      "99          P1_lag_1   3228.310119   2720\n",
      "145        E1_div_E2   3221.223163   2818\n",
      "36                I8   3195.449479   2943\n",
      "41               M12   3169.123804   3115\n",
      "143        M1_div_M2   3153.132757   2453\n",
      "32                I4   3142.651341   3221\n",
      "79                S8   3138.852117   2559\n",
      "108    M1_roll_std_5   3115.033341   2494\n",
      "15               E15   3107.619292   2437\n",
      "57               P10   2783.629208   2403\n",
      "28                E9   2769.625080   2574\n",
      "56                P1   2736.570986   2376\n",
      "18               E18   2614.323675   2012\n",
      "58               P11   2602.370089   2019\n",
      "52                M6   2589.979546   2288\n",
      "152      P1_minus_P2   2568.469280   2165\n",
      "10               E10   2492.679369   1967\n",
      "90                V6   2469.845243   1928\n",
      "67                P8   2456.404992   2191\n",
      "22                E3   2419.144209   1926\n",
      "120   M1_roll_std_10   2408.094211   1993\n",
      "51                M5   2384.561977   2091\n",
      "146      E1_minus_E2   2339.158598   1864\n",
      "16               E16   2320.549866   1871\n",
      "46               M17   2312.356408   1999\n",
      "20                E2   2306.317865   1912\n",
      "144      M1_minus_M2   2305.669698   2056\n",
      "112    V1_roll_std_5   2270.132304   1860\n",
      "24                E5   2160.168659   1829\n",
      "17               E17   2111.449425   1959\n",
      "47               M18   2018.538168   1717\n",
      "140        S1_ema_30   1969.531094   1311\n",
      "61                P2   1901.848072   1650\n",
      "110    E1_roll_std_5   1866.847746   1603\n",
      "21               E20   1854.520772   1587\n",
      "84               V12   1845.851870   1476\n",
      "74                S3   1820.321754   1502\n",
      "122   E1_roll_std_10   1771.208619   1508\n",
      "33                I5   1614.391342   1541\n",
      "69                S1   1539.326045   1192\n",
      "124   V1_roll_std_10   1510.845068   1225\n",
      "104         S1_lag_5   1495.910707   1218\n",
      "141        P1_ema_30   1459.885549   1158\n",
      "127  P1_roll_mean_10   1459.506867   1018\n",
      "103         V1_lag_5   1418.965401   1009\n",
      "37                I9   1401.732272   1234\n",
      "155            M1_sq   1371.090335   1152\n",
      "93                V9   1346.955876   1032\n",
      "115   P1_roll_mean_5   1312.345264   1089\n",
      "101         M1_lag_5   1149.083330   1014\n",
      "97          V1_lag_1   1084.981944    770\n",
      "98          S1_lag_1   1000.398201    955\n",
      "92                V8    920.756820    724\n",
      "113   S1_roll_mean_5    900.025789    880\n",
      "138        E1_ema_30    889.810587    838\n",
      "95          M1_lag_1    887.329507    725\n",
      "9                 E1    879.661684    759\n",
      "81                V1    873.723392    672\n",
      "135        P1_ema_10    851.778221    767\n",
      "42               M13    851.673454    620\n",
      "134        S1_ema_10    811.440611    693\n",
      "125  S1_roll_mean_10    803.910148    803\n",
      "38                M1    777.164889    676\n",
      "83               V11    760.191110    641\n",
      "45               M16    682.734993    584\n",
      "43               M14    677.944415    659\n",
      "137        M1_ema_30    654.469160    517\n",
      "96          E1_lag_1    652.903319    553\n",
      "119  M1_roll_mean_10    625.844242    441\n",
      "26                E7    614.418809    543\n",
      "102         E1_lag_5    603.964271    527\n",
      "111   V1_roll_mean_5    602.750957    473\n",
      "159            P1_sq    581.286204    479\n",
      "106         D1_lag_5    579.017792    285\n",
      "7                 D8    575.514928    362\n",
      "139        V1_ema_30    552.770068    369\n",
      "107   M1_roll_mean_5    550.805969    460\n",
      "82               V10    533.977651    452\n",
      "123  V1_roll_mean_10    498.536590    347\n",
      "5                 D6    490.618318    280\n",
      "3                 D4    434.959298    323\n",
      "121  E1_roll_mean_10    390.782439    387\n",
      "133        V1_ema_10    368.803859    235\n",
      "8                 D9    344.902743    306\n",
      "4                 D5    322.529281    315\n",
      "131        M1_ema_10    321.608096    250\n",
      "109   E1_roll_mean_5    287.250503    255\n",
      "132        E1_ema_10    249.021775    214\n",
      "157            V1_sq    246.442773    171\n",
      "156            E1_sq    204.597294    148\n",
      "6                 D7    172.774046     97\n",
      "2                 D3     78.305522     69\n",
      "100         D1_lag_1     70.647588     51\n",
      "129  D1_roll_mean_10     67.776511     47\n",
      "117   D1_roll_mean_5     35.896068     20\n",
      "118    D1_roll_std_5     16.429852     10\n",
      "130   D1_roll_std_10      5.364088      6\n",
      "0                 D1      4.043390      1\n",
      "1                 D2      0.000000      0\n",
      "154      D1_minus_D2      0.000000      0\n",
      "153        D1_div_D2      0.000000      0\n",
      "160            D1_sq      0.000000      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    \"feature\": MODEL_LGBM.booster_.feature_name(),\n",
    "    \"gain\": MODEL_LGBM.booster_.feature_importance(importance_type=\"gain\"),\n",
    "    \"split\": MODEL_LGBM.booster_.feature_importance(importance_type=\"split\"),\n",
    "})\n",
    "\n",
    "importance = importance.sort_values(\"gain\", ascending=False)\n",
    "\n",
    "print(importance.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c382c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd5039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4524eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3532d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "def strategy_sharpe(y_true, allocation):\n",
    "    # allocation in [0, 2]; realized return = allocation * excess_return\n",
    "    r = allocation * y_true\n",
    "    return np.mean(r) / (np.std(r) + 1e-9)\n",
    "\n",
    "# Load and preprocess data\n",
    "train_path = os.path.expanduser('~/Documents/kaggle/hull_tactical/data/train.csv')\n",
    "train_df = pl.read_csv(train_path, try_parse_dates=True, infer_schema_length=100000)\n",
    "train_df = train_df.with_columns(pl.all().exclude('date_id').cast(pl.Float64, strict=False))\n",
    "\n",
    "y = train_df['market_forward_excess_returns'].to_numpy()\n",
    "X_pd = preprocess_data(train_df, is_training=True).fillna(0)  # uses your existing function\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "scores = []\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(tscv.split(X_pd)):\n",
    "    X_train, X_val = X_pd.iloc[train_idx], X_pd.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Binary target for classifiers\n",
    "    y_train_bin = (y_train > 0).astype(int)\n",
    "\n",
    "    # --- LightGBM ---\n",
    "    model_lgbm = lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=127,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        reg_lambda=3.0,\n",
    "        reg_alpha=0.5,\n",
    "        min_child_samples=20,\n",
    "    )\n",
    "    model_lgbm.fit(X_train, y_train_bin)\n",
    "\n",
    "    # --- XGBoost ---\n",
    "    model_xgb = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        n_estimators=1200,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=7,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=0.2,\n",
    "        reg_lambda=3.0,\n",
    "        reg_alpha=0.5,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "    )\n",
    "    model_xgb.fit(X_train, y_train_bin)\n",
    "\n",
    "    # Blend predictions (probability of positive return)\n",
    "    p_lgbm = model_lgbm.predict_proba(X_val)[:, 1]\n",
    "    p_xgb  = model_xgb.predict_proba(X_val)[:, 1]\n",
    "    p_blend = 0.6 * p_lgbm + 0.4 * p_xgb\n",
    "\n",
    "    # Convert blended probability to allocation in [0, 2]\n",
    "    allocation = 1.0 + np.tanh(3.0 * (p_blend - 0.5))\n",
    "    allocation = np.clip(allocation, 0.0, 2.0)\n",
    "\n",
    "    # Use strategy Sharpe (allocation * excess_return)\n",
    "    score = strategy_sharpe(y_val, allocation)\n",
    "    scores.append(score)\n",
    "    print(f\"Fold {i+1} Strategy-Sharpe: {score:.6f}\")\n",
    "\n",
    "print(\"Average Strategy-Sharpe CV:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6911be0",
   "metadata": {
    "papermill": {
     "duration": 0.010517,
     "end_time": "2025-11-11T19:47:33.163931",
     "exception": false,
     "start_time": "2025-11-11T19:47:33.153414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # --- Main Execution Block for Kaggle ---\n",
    "\n",
    "# # The DefaultInferenceServer handles the time-series API interaction. \n",
    "# inference_server_instance = inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "# # Run the server logic based on the environment\n",
    "# if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#     inference_server_instance.serve()\n",
    "# else:\n",
    "#     logging.info(\"Running local gateway for testing.\")\n",
    "#     try:\n",
    "#         local_input_path = os.path.join(os.getcwd(), 'kaggle_input/hull-tactical-market-prediction/')\n",
    "#         inference_server_instance.run_local_gateway((local_input_path,))\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Local gateway simulation failed. This is expected outside Kaggle: {e}\")\n",
    "        \n",
    "#         # Run the expanded mock test and visualization\n",
    "#         run_mock_test_and_visualize()\n",
    "        \n",
    "#         # --- Mandatory Dummy Submission File Generation for Kaggle System Check ---\n",
    "#         logging.info(\"Generating dummy submission.parquet for Kaggle system check.\")\n",
    "        \n",
    "#         dummy_submission = pl.DataFrame({\n",
    "#             'date_id': [999], \n",
    "#             'allocation': [1.0]\n",
    "#         })\n",
    "#         dummy_submission.write_parquet('submission.parquet')\n",
    "#         logging.info(\"submission.parquet created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path.home() / \"Documents/kaggle/hull_tactical/data\"\n",
    "\n",
    "train = pl.read_csv(DATA_PATH / \"train.csv\")\n",
    "test  = pl.read_csv(DATA_PATH / \"test.csv\")\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "print(train.columns[:20])\n",
    "train.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950188c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast all non-date columns to float where possible\n",
    "train = train.with_columns(\n",
    "    pl.all().exclude(\"date_id\").cast(pl.Float64, strict=False)\n",
    ")\n",
    "\n",
    "# confirm types\n",
    "train.dtypes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94905e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b878ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "null_counts = (\n",
    "    train\n",
    "    .select(pl.all().null_count())                                 # one-row DF of counts\n",
    "    .transpose(include_header=True, header_name=\"column\", column_names=[\"nulls\"])\n",
    "    .sort(\"nulls\", descending=True)\n",
    ")\n",
    "\n",
    "null_counts.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "N = train.height\n",
    "THRESH = int(0.5 * N)  # drop columns with >50% nulls\n",
    "\n",
    "# 1) null counts as Series\n",
    "nc = (\n",
    "    train\n",
    "    .select(pl.all().null_count())\n",
    "    .transpose(include_header=True, header_name=\"column\", column_names=[\"nulls\"])\n",
    ")\n",
    "\n",
    "# 2) keep/drop lists\n",
    "to_keep = nc.filter(pl.col(\"nulls\") <= THRESH)[\"column\"].to_list()\n",
    "to_drop = nc.filter(pl.col(\"nulls\") > THRESH)[\"column\"].to_list()\n",
    "\n",
    "# 3) always keep these even if nully (index/targets)\n",
    "ALWAYS_KEEP = {\"date_id\", \"market_forward_excess_returns\"}\n",
    "to_keep = list(sorted(set(to_keep).union(ALWAYS_KEEP)))\n",
    "to_drop = [c for c in to_drop if c not in ALWAYS_KEEP]\n",
    "\n",
    "print(f\"keep: {len(to_keep)} cols | drop: {len(to_drop)} cols\")\n",
    "# peek which got dropped\n",
    "to_drop[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28aa172",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filled = train.select([\n",
    "    pl.when(pl.col(c).is_null())\n",
    "    .then(pl.col(c).mean())\n",
    "    .otherwise(pl.col(c))\n",
    "    .cast(pl.Float64)\n",
    "    .alias(c)\n",
    "    for c in to_keep\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filled.dtypes[:10]\n",
    "train_filled.null_count().sum()\n",
    "train_filled.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select([\n",
    "    pl.col(\"market_forward_excess_returns\").mean().alias(\"mean\"),\n",
    "    pl.col(\"market_forward_excess_returns\").std().alias(\"std\"),\n",
    "    pl.col(\"market_forward_excess_returns\").min().alias(\"min\"),\n",
    "    pl.col(\"market_forward_excess_returns\").max().alias(\"max\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2982c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(\n",
    "    train_filled[\"market_forward_excess_returns\"].to_numpy(),\n",
    "    bins=50,\n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "plt.title(\"Distribution of Market Forward Excess Returns\")\n",
    "plt.xlabel(\"Return\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ebb1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "y = train[\"market_forward_excess_returns\"].to_numpy()\n",
    "X = train.drop([\"market_forward_excess_returns\"]).to_pandas()\n",
    "\n",
    "# Fill any remaining NaNs\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "    print(f\"Fold {i+1}: train={train_idx[0]}–{train_idx[-1]}, val={val_idx[0]}–{val_idx[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_like(y_true, y_pred):\n",
    "    r = y_pred - y_true\n",
    "    return np.mean(r) / (np.std(r) + 1e-9)\n",
    "\n",
    "mean_pred = np.full_like(y, y.mean())\n",
    "\n",
    "scores = []\n",
    "for i, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "    y_val = y[val_idx]\n",
    "    y_pred = mean_pred[val_idx]\n",
    "    score = sharpe_like(y_val, y_pred)\n",
    "    scores.append(score)\n",
    "    print(f\"Fold {i+1} Sharpe-like: {score:.6f}\")\n",
    "\n",
    "print(\"Average Sharpe-like CV:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28612464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "enet = ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=100000)\n",
    "\n",
    "scores = []\n",
    "for i, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    enet.fit(X_train, y_train)\n",
    "    y_pred = enet.predict(X_val)\n",
    "    score = sharpe_like(y_val, y_pred)\n",
    "    scores.append(score)\n",
    "    print(f\"Fold {i+1} Sharpe-like: {score:.6f}\")\n",
    "\n",
    "print(\"Average Sharpe-like CV:\", np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14348714,
     "isSourceIdPinned": false,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 214.560118,
   "end_time": "2025-11-11T19:47:34.299593",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-11T19:43:59.739475",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
